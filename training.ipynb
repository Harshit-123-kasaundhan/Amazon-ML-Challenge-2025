{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c13aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74999 rows\n",
      "Columns in temp_train.csv: ['sample_id', 'product_name', 'catalog_content_clean', 'price', 'value', 'unit', 'image']\n",
      "Train: 59999 rows, Test: 15000 rows\n",
      "DataLoaders ready with your dataset structure.\n",
      "Batch loaded: 16 images, 16 texts, torch.Size([16]) prices\n",
      "Image type: <class 'PIL.Image.Image'>, Price type: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define Custom Dataset and DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Load and split data\n",
    "# DATASET_FOLDER = '/content/drive/MyDrive/temp_dataset'\n",
    "IMAGE_FOLDER = 'images/train'\n",
    "df = pd.read_csv(\"dataset/merged_train_with_image.csv\")\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(\"Columns in temp_train.csv:\", df.columns.tolist())\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {len(train_df)} rows, Test: {len(test_df)} rows\")\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle PIL Images, strings, and prices.\n",
    "    Returns images as a list, texts as a list, and prices as a tensor.\n",
    "    \"\"\"\n",
    "    images, texts, prices = zip(*batch)\n",
    "    return list(images), list(texts), torch.tensor(prices, dtype=torch.float32)\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, df, image_folder):\n",
    "        \"\"\"\n",
    "        Initialize dataset with DataFrame and image folder.\n",
    "        - df: Contains [sample_id, product_name, catalog_content_clean, price, value, unit, image]\n",
    "        - image_folder: Directory with images named in 'image' column (e.g., 33127.jpg)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = None  # ViTImageProcessor handles preprocessing\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, row['image'])  # Use 'image' column\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found at {image_path}, using placeholder.\")\n",
    "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        text = row['catalog_content_clean']\n",
    "        price = float(row['price'])\n",
    "        return img, text, price\n",
    "\n",
    "# Create and test DataLoaders\n",
    "try:\n",
    "    train_dataset = ProductDataset(train_df, IMAGE_FOLDER)\n",
    "    test_dataset = ProductDataset(test_df, IMAGE_FOLDER)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, num_workers=0, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    print(\"DataLoaders ready with your dataset structure.\")\n",
    "    # Test one batch\n",
    "    for batch_images, batch_texts, batch_prices in train_loader:\n",
    "        print(f\"Batch loaded: {len(batch_images)} images, {len(batch_texts)} texts, {batch_prices.shape} prices\")\n",
    "        print(f\"Image type: {type(batch_images[0])}, Price type: {type(batch_prices)}\")\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(f\"Error creating DataLoaders: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e35de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.0 MB 8.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.8/12.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.4/12.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.0 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 11.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "   ---------------------------------------- 0.0/564.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 564.3/564.3 kB 9.7 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 2.4/2.7 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 11.9 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.35.3 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.57.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372d05c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders fine-tuning: True\n",
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define Multimodal Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTImageProcessor, ViTModel, BertTokenizer, BertModel, DistilBertModel\n",
    "\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, fine_tune_encoders=True):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.image_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.text_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.text_proj = nn.Linear(768, 512)\n",
    "        self.image_proj = nn.Linear(768, 512)\n",
    "        # Add projection layer to reduce 1024-dim (512+512) to 768-dim for DistilBERT\n",
    "        self.concat_proj = nn.Linear(1024, 768)\n",
    "        self.transformer = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if not fine_tune_encoders:\n",
    "            for param in self.image_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.text_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        print(\"Encoders fine-tuning:\", fine_tune_encoders)\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        # Image encoding\n",
    "        inputs = self.image_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        image_outputs = self.image_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        image_proj = F.normalize(self.image_proj(image_outputs), dim=-1)\n",
    "        # Text encoding\n",
    "        inputs = self.text_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "        text_outputs = self.text_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        text_proj = F.normalize(self.text_proj(text_outputs), dim=-1)\n",
    "        # Concatenate and project to 768-dim\n",
    "        concat_emb = torch.cat([text_proj, image_proj], dim=-1)  # [batch_size, 1024]\n",
    "        concat_emb = self.concat_proj(concat_emb).unsqueeze(1)  # [batch_size, 1, 768]\n",
    "        # Transformer\n",
    "        transformer_out = self.transformer(inputs_embeds=concat_emb).last_hidden_state.mean(dim=1)\n",
    "        # Regression\n",
    "        price = self.regression(transformer_out)\n",
    "        return price.squeeze(), text_proj, image_proj\n",
    "device = torch.device('cuda')  # Define device\n",
    "model = MultimodalModel(fine_tune_encoders=True).to(device)\n",
    "print(\"Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db104fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Epoch 1/100 - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2133/3750 [4:59:19<4:11:01,  9.31s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import os  # Add this import for os.path operations\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Define Losses ---\n",
    "def contrastive_loss(text_proj, image_proj, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Contrastive loss to align image and text projections.\n",
    "    \"\"\"\n",
    "    similarity = text_proj @ image_proj.T / temperature\n",
    "    labels = torch.arange(text_proj.size(0)).to(device)\n",
    "    return F.cross_entropy(similarity, labels)\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error for evaluation.\n",
    "    \"\"\"\n",
    "    y_true = y_true.cpu().numpy() if isinstance(y_true, torch.Tensor) else np.array(y_true)\n",
    "    y_pred = y_pred.cpu().numpy() if isinstance(y_pred, torch.Tensor) else np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_pred - y_true) / np.maximum(denominator, 1e-8)  # avoid /0\n",
    "    return np.mean(diff) * 100\n",
    "\n",
    "# --- Optimizer and Loss ---\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "start_time = time.time()\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ------------------- TRAINING -------------------\n",
    "    model.train()\n",
    "    train_mse, train_cont = 0, 0\n",
    "    train_preds, train_true = [], []\n",
    "\n",
    "    print(f\"\\nðŸ”¹ Epoch {epoch + 1}/{num_epochs} - Training\")\n",
    "    for batch_images, batch_texts, batch_prices in tqdm(train_loader, desc=\"Training Progress\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        pred, text_proj, image_proj = model(batch_images, batch_texts)\n",
    "\n",
    "        mse = criterion(pred, batch_prices.to(device))\n",
    "        cont = contrastive_loss(text_proj, image_proj)\n",
    "\n",
    "        loss = mse + 0.5 * cont\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_mse += mse.item()\n",
    "        train_cont += cont.item()\n",
    "        train_preds.extend(pred.cpu().detach().numpy())\n",
    "        train_true.extend(batch_prices.cpu().numpy())\n",
    "\n",
    "    avg_train_mse = train_mse / len(train_loader)\n",
    "    avg_train_cont = train_cont / len(train_loader)\n",
    "    train_smape = smape(np.array(train_true), np.array(train_preds))\n",
    "    print(f\"âœ… Train MSE = {avg_train_mse:.4f}, Train Cont = {avg_train_cont:.4f}, Train SMAPE = {train_smape:.2f}%\")\n",
    "\n",
    "    # ------------------- TESTING -------------------\n",
    "    model.eval()\n",
    "    test_mse, test_cont = 0, 0\n",
    "    test_preds, test_true = [], []\n",
    "\n",
    "    print(f\"ðŸ§ª Epoch {epoch + 1}/{num_epochs} - Testing\")\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_texts, batch_prices in tqdm(test_loader, desc=\"Testing Progress\", leave=False):\n",
    "            pred, text_proj, image_proj = model(batch_images, batch_texts)\n",
    "            mse = criterion(pred, batch_prices.to(device))\n",
    "            cont = contrastive_loss(text_proj, image_proj)\n",
    "            test_mse += mse.item()\n",
    "            test_cont += cont.item()\n",
    "\n",
    "            if pred.dim() == 0:\n",
    "                test_preds.append(pred.cpu().numpy().item())\n",
    "            else:\n",
    "                test_preds.extend(pred.cpu().numpy())\n",
    "            test_true.extend(batch_prices.cpu().numpy())\n",
    "\n",
    "    avg_test_mse = test_mse / len(test_loader)\n",
    "    avg_test_cont = test_cont / len(test_loader)\n",
    "    test_smape = smape(np.array(test_true), np.array(test_preds))\n",
    "    print(f\"ðŸ“Š Test MSE = {avg_test_mse:.4f}, Test Cont = {avg_test_cont:.4f}, Test SMAPE = {test_smape:.2f}%\")\n",
    "\n",
    "    # --- Save Model Per Epoch ---\n",
    "    model_save_path = os.path.join(pth, f'multimodal_model_epoch_{epoch + 1}.pth')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"ðŸ’¾ Model saved for epoch {epoch + 1} at {model_save_path}\")\n",
    "\n",
    "# --- Final Message ---\n",
    "print(f\"\\nðŸ Training completed in {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b657b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT model initialized.\n",
      "BERT model initialized.\n",
      "Loaded 74999 rows\n",
      "Columns in CSV: ['sample_id', 'product_name', 'catalog_content_clean', 'value', 'unit', 'image']\n",
      "Train: 59999 rows, Test: 15000 rows\n",
      "DataLoaders ready.\n",
      "Batch loaded: 32 images, 32 texts, torch.Size([32]) prices\n",
      "Image type: <class 'PIL.Image.Image'>, Price type: <class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   3%|â–Ž         | 64/1875 [02:42<1:16:31,  2.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 191\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Extract and save embeddings for train and test\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m \u001b[43mextract_and_save_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEMBEDDINGS_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\n\u001b[0;32m    194\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m extract_and_save_embeddings(\n\u001b[0;32m    196\u001b[0m     image_model, text_model, image_processor, text_tokenizer,\n\u001b[0;32m    197\u001b[0m     test_loader, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(EMBEDDINGS_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m\n\u001b[0;32m    198\u001b[0m )\n",
      "Cell \u001b[1;32mIn[4], line 72\u001b[0m, in \u001b[0;36mextract_and_save_embeddings\u001b[1;34m(image_model, text_model, image_processor, text_tokenizer, data_loader, save_dir, max_length)\u001b[0m\n\u001b[0;32m     69\u001b[0m all_image_paths \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (batch_images, batch_texts, batch_prices) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(data_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;66;03m# Image embeddings (mean-pooled)\u001b[39;00m\n\u001b[0;32m     74\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m image_processor(images\u001b[38;5;241m=\u001b[39mbatch_images, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     75\u001b[0m         image_emb \u001b[38;5;241m=\u001b[39m image_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, 768]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 39\u001b[0m, in \u001b[0;36mProductDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     37\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_folder, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, using placeholder.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\PIL\\Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    876\u001b[0m ):\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import ViTImageProcessor, ViTModel, BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# --- Custom Dataset and Collate Function ---\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle PIL Images, strings, and prices.\n",
    "    Returns images as a list, texts as a list, prices as a tensor.\n",
    "    \"\"\"\n",
    "    images, texts, prices = zip(*batch)\n",
    "    return list(images), list(texts), torch.tensor(prices, dtype=torch.float32)\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, df, image_folder):\n",
    "        \"\"\"\n",
    "        Initialize dataset with DataFrame and image folder.\n",
    "        - df: Contains [sample_id, product_name, catalog_content_clean, price, value, unit, image]\n",
    "        - image_folder: Directory with images named in 'image' column (e.g., 33127.jpg)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = None  # ViTImageProcessor handles preprocessing\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, row['image'])\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found at {image_path}, using placeholder.\")\n",
    "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        text = row['catalog_content_clean']\n",
    "        price = float(row['value'])  # Changed 'price' to 'value' (or replace with correct column name)\n",
    "        return img, text, price\n",
    "\n",
    "# --- Function to Extract and Save Embeddings ---\n",
    "def extract_and_save_embeddings(image_model, text_model, image_processor, text_tokenizer, data_loader, save_dir, max_length=128):\n",
    "    \"\"\"\n",
    "    Extract mean-pooled ViT and BERT embeddings and save them with prices and metadata.\n",
    "    \n",
    "    Args:\n",
    "        image_model: ViT model.\n",
    "        text_model: BERT model.\n",
    "        image_processor: ViT processor.\n",
    "        text_tokenizer: BERT tokenizer.\n",
    "        data_loader: DataLoader providing (images, texts, prices).\n",
    "        save_dir: Directory to save embeddings (e.g., './embeddings/').\n",
    "        max_length: Max sequence length for text (default: 128).\n",
    "    \"\"\"\n",
    "    image_model.eval()\n",
    "    text_model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    all_prices = []\n",
    "    all_sample_ids = []\n",
    "    all_image_paths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch_images, batch_texts, batch_prices) in enumerate(tqdm(data_loader, desc=\"Extracting embeddings\")):\n",
    "            # Image embeddings (mean-pooled)\n",
    "            inputs = image_processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "            image_emb = image_model(**inputs).last_hidden_state.mean(dim=1)  # [batch_size, 768]\n",
    "            all_image_embeddings.append(image_emb.cpu())\n",
    "            \n",
    "            # Text embeddings (mean-pooled)\n",
    "            inputs = text_tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "            text_emb = text_model(**inputs).last_hidden_state.mean(dim=1)  # [batch_size, 768]\n",
    "            all_text_embeddings.append(text_emb.cpu())\n",
    "            \n",
    "            # Store prices and metadata\n",
    "            all_prices.append(batch_prices)\n",
    "            batch_start_idx = batch_idx * data_loader.batch_size\n",
    "            batch_end_idx = min(batch_start_idx + len(batch_images), len(data_loader.dataset))\n",
    "            all_sample_ids.extend(data_loader.dataset.df.iloc[batch_start_idx:batch_end_idx]['sample_id'].tolist())\n",
    "            all_image_paths.extend([os.path.join(data_loader.dataset.image_folder, data_loader.dataset.df.iloc[i]['image']) \n",
    "                                   for i in range(batch_start_idx, batch_end_idx)])\n",
    "\n",
    "    # Concatenate all\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)  # [num_samples, 768]\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)    # [num_samples, 768]\n",
    "    all_prices = torch.cat(all_prices, dim=0)                      # [num_samples]\n",
    "    \n",
    "    # Save embeddings and prices\n",
    "    torch.save({\n",
    "        'image_embeddings': all_image_embeddings,\n",
    "        'text_embeddings': all_text_embeddings,\n",
    "        'prices': all_prices\n",
    "    }, os.path.join(save_dir, 'embeddings.pt'))\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = pd.DataFrame({\n",
    "        'sample_id': all_sample_ids,\n",
    "        'image_path': all_image_paths\n",
    "    })\n",
    "    metadata.to_csv(os.path.join(save_dir, 'metadata.csv'), index=False)\n",
    "    \n",
    "    print(f\"âœ… Embeddings saved to {save_dir}\")\n",
    "    print(f\"Image embeddings shape: {all_image_embeddings.shape} ([num_samples, 768])\")\n",
    "    print(f\"Text embeddings shape: {all_text_embeddings.shape} ([num_samples, 768])\")\n",
    "    print(f\"Prices shape: {all_prices.shape} ([num_samples])\")\n",
    "\n",
    "# --- Function to Load Saved Embeddings ---\n",
    "def load_embeddings(save_dir):\n",
    "    \"\"\"\n",
    "    Load saved embeddings, prices, and metadata from disk.\n",
    "    \n",
    "    Args:\n",
    "        save_dir: Directory where embeddings are saved.\n",
    "    \n",
    "    Returns:\n",
    "        image_embeddings: Tensor of shape [num_samples, 768]\n",
    "        text_embeddings: Tensor of shape [num_samples, 768]\n",
    "        prices: Tensor of shape [num_samples]\n",
    "        metadata: DataFrame with sample_id, image_path\n",
    "    \"\"\"\n",
    "    data = torch.load(os.path.join(save_dir, 'embeddings.pt'))\n",
    "    image_embeddings = data['image_embeddings']\n",
    "    text_embeddings = data['text_embeddings']\n",
    "    prices = data['prices']\n",
    "    metadata = pd.read_csv(os.path.join(save_dir, 'metadata.csv'))\n",
    "    print(f\"Loaded embeddings:\")\n",
    "    print(f\"Image embeddings shape: {image_embeddings.shape} ([num_samples, 768])\")\n",
    "    print(f\"Text embeddings shape: {text_embeddings.shape} ([num_samples, 768])\")\n",
    "    print(f\"Prices shape: {prices.shape} ([num_samples])\")\n",
    "    return image_embeddings, text_embeddings, prices, metadata\n",
    "\n",
    "# --- Main Script ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load ViT for images\n",
    "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "image_model = ViTModel.from_pretrained('google/vit-base-patch16-224').to(device)\n",
    "print(\"ViT model initialized.\")\n",
    "\n",
    "# Load BERT for text\n",
    "text_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "print(\"BERT model initialized.\")\n",
    "\n",
    "# Define embeddings directory\n",
    "EMBEDDINGS_DIR = './embeddings/'\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "IMAGE_FOLDER = 'test_images/test_images'\n",
    "df = pd.read_csv(\"test/merged_test_with_image.csv\")\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(\"Columns in CSV:\", df.columns.tolist())\n",
    "\n",
    "# Verify required columns\n",
    "required_columns = ['sample_id', 'catalog_content_clean', 'value', 'image']  # Adjust 'value' to actual price column\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns in CSV: {missing_columns}\")\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {len(train_df)} rows, Test: {len(test_df)} rows\")\n",
    "\n",
    "# Create DataLoaders for train and test\n",
    "try:\n",
    "    train_dataset = ProductDataset(train_df, IMAGE_FOLDER)\n",
    "    test_dataset = ProductDataset(test_df, IMAGE_FOLDER)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    print(\"DataLoaders ready.\")\n",
    "    # Test one batch\n",
    "    for batch_images, batch_texts, batch_prices in train_loader:\n",
    "        print(f\"Batch loaded: {len(batch_images)} images, {len(batch_texts)} texts, {batch_prices.shape} prices\")\n",
    "        print(f\"Image type: {type(batch_images[0])}, Price type: {type(batch_prices)}\")\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(f\"Error creating DataLoader: {e}\")\n",
    "    raise\n",
    "\n",
    "# Extract and save embeddings for train and test\n",
    "extract_and_save_embeddings(\n",
    "    image_model, text_model, image_processor, text_tokenizer,\n",
    "    train_loader, os.path.join(EMBEDDINGS_DIR, 'train'), max_length=128\n",
    ")\n",
    "extract_and_save_embeddings(\n",
    "    image_model, text_model, image_processor, text_tokenizer,\n",
    "    test_loader, os.path.join(EMBEDDINGS_DIR, 'test'), max_length=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e93fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import ViTImageProcessor, ViTModel, BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# --- Custom Dataset and Collate Function ---\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle PIL Images, strings, and prices.\n",
    "    Returns images as a list, texts as a list, prices as a tensor.\n",
    "    \"\"\"\n",
    "    images, texts, prices = zip(*batch)\n",
    "    return list(images), list(texts), torch.tensor(prices, dtype=torch.float32)\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, df, image_folder):\n",
    "        \"\"\"\n",
    "        Initialize dataset with DataFrame and image folder.\n",
    "        - df: Contains [sample_id, product_name, catalog_content_clean, value, unit, image]\n",
    "        - image_folder: Directory with images named in 'image' column (e.g., 33127.jpg)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = None  # ViTImageProcessor handles preprocessing\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, row['image'])\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found at {image_path}, using placeholder.\")\n",
    "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        text = row['catalog_content_clean']\n",
    "        price = float(row['value'])  # Using 'value' as the price column\n",
    "        return img, text, price\n",
    "\n",
    "# --- Function to Extract and Save Embeddings ---\n",
    "def extract_and_save_embeddings(image_model, text_model, image_processor, text_tokenizer, data_loader, save_dir, max_length=128):\n",
    "    \"\"\"\n",
    "    Extract mean-pooled ViT and BERT embeddings and save them with prices and metadata.\n",
    "    \n",
    "    Args:\n",
    "        image_model: ViT model.\n",
    "        text_model: BERT model.\n",
    "        image_processor: ViT processor.\n",
    "        text_tokenizer: BERT tokenizer.\n",
    "        data_loader: DataLoader providing (images, texts, prices).\n",
    "        save_dir: Directory to save embeddings (e.g., './embeddings/train/').\n",
    "        max_length: Max sequence length for text (default: 128).\n",
    "    \"\"\"\n",
    "    image_model.eval()\n",
    "    text_model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    all_prices = []\n",
    "    all_sample_ids = []\n",
    "    all_image_paths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch_images, batch_texts, batch_prices) in enumerate(tqdm(data_loader, desc=\"Extracting embeddings\")):\n",
    "            # Image embeddings (mean-pooled)\n",
    "            inputs = image_processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "            image_emb = image_model(**inputs).last_hidden_state.mean(dim=1)  # [batch_size, 768]\n",
    "            all_image_embeddings.append(image_emb.cpu())\n",
    "            \n",
    "            # Text embeddings (mean-pooled)\n",
    "            inputs = text_tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "            text_emb = text_model(**inputs).last_hidden_state.mean(dim=1)  # [batch_size, 768]\n",
    "            all_text_embeddings.append(text_emb.cpu())\n",
    "            \n",
    "            # Store prices and metadata\n",
    "            all_prices.append(batch_prices)\n",
    "            batch_start_idx = batch_idx * data_loader.batch_size\n",
    "            batch_end_idx = min(batch_start_idx + len(batch_images), len(data_loader.dataset))\n",
    "            all_sample_ids.extend(data_loader.dataset.df.iloc[batch_start_idx:batch_end_idx]['sample_id'].tolist())\n",
    "            all_image_paths.extend([os.path.join(data_loader.dataset.image_folder, data_loader.dataset.df.iloc[i]['image']) \n",
    "                                   for i in range(batch_start_idx, batch_end_idx)])\n",
    "\n",
    "    # Concatenate all\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)  # [num_samples, 768]\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)    # [num_samples, 768]\n",
    "    all_prices = torch.cat(all_prices, dim=0)                      # [num_samples]\n",
    "    \n",
    "    # Save embeddings and prices\n",
    "    torch.save({\n",
    "        'image_embeddings': all_image_embeddings,\n",
    "        'text_embeddings': all_text_embeddings,\n",
    "        'prices': all_prices\n",
    "    }, os.path.join(save_dir, 'embeddings.pt'))\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = pd.DataFrame({\n",
    "        'sample_id': all_sample_ids,\n",
    "        'image_path': all_image_paths\n",
    "    })\n",
    "    metadata.to_csv(os.path.join(save_dir, 'metadata.csv'), index=False)\n",
    "    \n",
    "    print(f\"âœ… Embeddings saved to {save_dir}\")\n",
    "    print(f\"Image embeddings shape: {all_image_embeddings.shape} ([num_samples, 768])\")\n",
    "    print(f\"Text embeddings shape: {all_text_embeddings.shape} ([num_samples, 768])\")\n",
    "    print(f\"Prices shape: {all_prices.shape} ([num_samples])\")\n",
    "\n",
    "# --- Function to Load Saved Embeddings ---\n",
    "def load_embeddings(save_dir):\n",
    "    \"\"\"\n",
    "    Load saved embeddings, prices, and metadata from disk.\n",
    "    \n",
    "    Args:\n",
    "        save_dir: Directory where embeddings are saved.\n",
    "    \n",
    "    Returns:\n",
    "        image_embeddings: Tensor of shape [num_samples, 768]\n",
    "        text_embeddings: Tensor of shape [num_samples, 768]\n",
    "        prices: Tensor of shape [num_samples]\n",
    "        metadata: DataFrame with sample_id, image_path\n",
    "    \"\"\"\n",
    "    data = torch.load(os.path.join(save_dir, 'embeddings.pt'))\n",
    "    image_embeddings = data['image_embeddings']\n",
    "    text_embeddings = data['text_embeddings']\n",
    "    prices = data['prices']\n",
    "    metadata = pd.read_csv(os.path.join(save_dir, 'metadata.csv'))\n",
    "    print(f\"Loaded embeddings:\")\n",
    "    print(f\"Image embeddings shape: {image_embeddings.shape} ([num_samples, 768])\")\n",
    "    print(f\"Text embeddings shape: {text_embeddings.shape} ([num_samples, 768])\")\n",
    "    print(f\"Prices shape: {prices.shape} ([num_samples])\")\n",
    "    return image_embeddings, text_embeddings, prices, metadata\n",
    "\n",
    "# --- Main Script ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load ViT for images\n",
    "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "image_model = ViTModel.from_pretrained('google/vit-base-patch16-224').to(device)\n",
    "print(\"ViT model initialized.\")\n",
    "\n",
    "# Load BERT for text\n",
    "text_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "print(\"BERT model initialized.\")\n",
    "\n",
    "# Define embeddings directory\n",
    "EMBEDDINGS_DIR = './embeddings/train/'\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "IMAGE_FOLDER = 'test_images/test_images'  # Assuming this is your training image folder\n",
    "df = pd.read_csv(\"test/merged_test_with_image.csv\")  # Assuming this is your training CSV\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(\"Columns in CSV:\", df.columns.tolist())\n",
    "\n",
    "# Verify required columns\n",
    "required_columns = ['sample_id', 'catalog_content_clean', 'value', 'image']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns in CSV: {missing_columns}\")\n",
    "\n",
    "# Create DataLoader for training data only\n",
    "try:\n",
    "    train_dataset = ProductDataset(df, IMAGE_FOLDER)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    print(\"DataLoader ready for training data.\")\n",
    "    # Test one batch\n",
    "    for batch_images, batch_texts, batch_prices in train_loader:\n",
    "        print(f\"Batch loaded: {len(batch_images)} images, {len(batch_texts)} texts, {batch_prices.shape} prices\")\n",
    "        print(f\"Image type: {type(batch_images[0])}, Price type: {type(batch_prices)}\")\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(f\"Error creating DataLoader: {e}\")\n",
    "    raise\n",
    "\n",
    "# Extract and save embeddings for training data only\n",
    "extract_and_save_embeddings(\n",
    "    image_model, text_model, image_processor, text_tokenizer,\n",
    "    train_loader, EMBEDDINGS_DIR, max_length=128\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "battery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
