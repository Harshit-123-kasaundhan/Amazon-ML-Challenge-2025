{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b90cb8-5021-4a5c-baa0-2716a87b5fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74999 rows\n",
      "Columns in temp_train.csv: ['sample_id', 'product_name', 'catalog_content_clean', 'price', 'value', 'unit', 'image']\n",
      "Train: 59999 rows, Test: 15000 rows\n",
      "DataLoaders ready with your dataset structure.\n",
      "Batch loaded: 16 images, 16 texts, torch.Size([16]) prices\n",
      "Image type: <class 'PIL.Image.Image'>, Price type: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define Custom Dataset and DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "\n",
    "IMAGE_FOLDER = 'images/train'\n",
    "df = pd.read_csv(\"dataset/merged_train_with_image.csv\")\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(\"Columns in temp_train.csv:\", df.columns.tolist())\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {len(train_df)} rows, Test: {len(test_df)} rows\")\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle PIL Images, strings, and prices.\n",
    "    Returns images as a list, texts as a list, and prices as a tensor.\n",
    "    \"\"\"\n",
    "    images, texts, prices = zip(*batch)\n",
    "    return list(images), list(texts), torch.tensor(prices, dtype=torch.float32)\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, df, image_folder):\n",
    "        \"\"\"\n",
    "        Initialize dataset with DataFrame and image folder.\n",
    "        - df: Contains [sample_id, product_name, catalog_content_clean, price, value, unit, image]\n",
    "        - image_folder: Directory with images named in 'image' column (e.g., 33127.jpg)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = None  # ViTImageProcessor handles preprocessing\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, row['image'])  # Use 'image' column\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found at {image_path}, using placeholder.\")\n",
    "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        text = row['catalog_content_clean']\n",
    "        price = float(row['price'])\n",
    "        return img, text, price\n",
    "\n",
    "# Create and test DataLoaders\n",
    "try:\n",
    "    train_dataset = ProductDataset(train_df, IMAGE_FOLDER)\n",
    "    test_dataset = ProductDataset(test_df, IMAGE_FOLDER)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, num_workers=0, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    print(\"DataLoaders ready with your dataset structure.\")\n",
    "    # Test one batch\n",
    "    for batch_images, batch_texts, batch_prices in train_loader:\n",
    "        print(f\"Batch loaded: {len(batch_images)} images, {len(batch_texts)} texts, {batch_prices.shape} prices\")\n",
    "        print(f\"Image type: {type(batch_images[0])}, Price type: {type(batch_prices)}\")\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(f\"Error creating DataLoaders: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63627622-1669-4a18-b2fa-db0a0065dfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/pranjal/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/pranjal/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/pranjal/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/pranjal/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/pranjal/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/pranjal/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d68481f-07e7-407c-bff2-bb3ea240b4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders fine-tuning: False\n",
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define Multimodal Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTImageProcessor, ViTModel, BertTokenizer, BertModel, DistilBertModel\n",
    "\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, fine_tune_encoders=True):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.image_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.text_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.text_proj = nn.Linear(768, 512)\n",
    "        self.image_proj = nn.Linear(768, 512)\n",
    "        # Add projection layer to reduce 1024-dim (512+512) to 768-dim for DistilBERT\n",
    "        self.concat_proj = nn.Linear(1024, 768)\n",
    "        self.transformer = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if not fine_tune_encoders:\n",
    "            for param in self.image_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.text_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        print(\"Encoders fine-tuning:\", fine_tune_encoders)\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        # Image encoding\n",
    "        inputs = self.image_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        image_outputs = self.image_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        image_proj = F.normalize(self.image_proj(image_outputs), dim=-1)\n",
    "        # Text encoding\n",
    "        inputs = self.text_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "        text_outputs = self.text_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        text_proj = F.normalize(self.text_proj(text_outputs), dim=-1)\n",
    "        # Concatenate and project to 768-dim\n",
    "        concat_emb = torch.cat([text_proj, image_proj], dim=-1)  # [batch_size, 1024]\n",
    "        concat_emb = self.concat_proj(concat_emb).unsqueeze(1)  # [batch_size, 1, 768]\n",
    "        # Transformer\n",
    "        transformer_out = self.transformer(inputs_embeds=concat_emb).last_hidden_state.mean(dim=1)\n",
    "        # Regression\n",
    "        price = self.regression(transformer_out)\n",
    "        return price.squeeze(), text_proj, image_proj\n",
    "device = torch.device('cuda')  # Define device\n",
    "model = MultimodalModel(fine_tune_encoders=False).to(device)\n",
    "print(\"Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d9600cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import os  # Add this import for os.path operations\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c510ad0-8d4a-4186-91cb-c982a8ae1520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Epoch 1/100 - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train MSE = 1016.5583, Train Cont = 2.2768, Train SMAPE = 70.43%\n",
      "üß™ Epoch 1/100 - Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test MSE = 880.1211, Test Cont = 2.2500, Test SMAPE = 67.41%\n",
      "üíæ Model saved for epoch 1 at train_images/multimodal_model_epoch_1.pth\n",
      "\n",
      "üîπ Epoch 2/100 - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train MSE = 1012.9527, Train Cont = 2.1931, Train SMAPE = 69.45%\n",
      "üß™ Epoch 2/100 - Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test MSE = 1056.8264, Test Cont = 2.1715, Test SMAPE = 78.97%\n",
      "üíæ Model saved for epoch 2 at train_images/multimodal_model_epoch_2.pth\n",
      "\n",
      "üîπ Epoch 3/100 - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train MSE = 1089.0373, Train Cont = 2.1872, Train SMAPE = 71.28%\n",
      "üß™ Epoch 3/100 - Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test MSE = 859.7275, Test Cont = 2.2194, Test SMAPE = 68.91%\n",
      "üíæ Model saved for epoch 3 at train_images/multimodal_model_epoch_3.pth\n",
      "\n",
      "üîπ Epoch 4/100 - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train MSE = 906.2997, Train Cont = 2.2208, Train SMAPE = 66.79%\n",
      "üß™ Epoch 4/100 - Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test MSE = 805.9354, Test Cont = 2.1953, Test SMAPE = 62.60%\n",
      "üíæ Model saved for epoch 4 at train_images/multimodal_model_epoch_4.pth\n",
      "\n",
      "üîπ Epoch 5/100 - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train MSE = 1087.1153, Train Cont = 2.2016, Train SMAPE = 69.69%\n",
      "üß™ Epoch 5/100 - Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test MSE = 1210.0062, Test Cont = 2.1981, Test SMAPE = 79.55%\n",
      "üíæ Model saved for epoch 5 at train_images/multimodal_model_epoch_5.pth\n",
      "\n",
      "üîπ Epoch 6/100 - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train MSE = 1109.2570, Train Cont = 2.2141, Train SMAPE = 71.78%\n",
      "üß™ Epoch 6/100 - Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test MSE = 762.2732, Test Cont = 2.1955, Test SMAPE = 63.85%\n",
      "üíæ Model saved for epoch 6 at train_images/multimodal_model_epoch_6.pth\n",
      "\n",
      "üîπ Epoch 7/100 - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_images, batch_texts, batch_prices \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Progress\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 44\u001b[0m     pred, text_proj, image_proj \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     mse \u001b[38;5;241m=\u001b[39m criterion(pred, batch_prices\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     47\u001b[0m     cont \u001b[38;5;241m=\u001b[39m contrastive_loss(text_proj, image_proj)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mMultimodalModel.forward\u001b[0;34m(self, images, texts)\u001b[0m\n\u001b[1;32m     36\u001b[0m image_proj \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_proj(image_outputs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Text encoding\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m text_proj \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_proj(text_outputs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_mamba_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:837\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 837\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    838\u001b[0m         k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(v\u001b[38;5;241m.\u001b[39mto) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    840\u001b[0m     }\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    842\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_mamba_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:838\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 838\u001b[0m         k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(v\u001b[38;5;241m.\u001b[39mto) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    840\u001b[0m     }\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    842\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import os  # Add this import for os.path operations\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Define Losses ---\n",
    "def contrastive_loss(text_proj, image_proj, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Contrastive loss to align image and text projections.\n",
    "    \"\"\"\n",
    "    similarity = text_proj @ image_proj.T / temperature\n",
    "    labels = torch.arange(text_proj.size(0)).to(device)\n",
    "    return F.cross_entropy(similarity, labels)\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error for evaluation.\n",
    "    \"\"\"\n",
    "    y_true = y_true.cpu().numpy() if isinstance(y_true, torch.Tensor) else np.array(y_true)\n",
    "    y_pred = y_pred.cpu().numpy() if isinstance(y_pred, torch.Tensor) else np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_pred - y_true) / np.maximum(denominator, 1e-8)  # avoid /0\n",
    "    return np.mean(diff) * 100\n",
    "\n",
    "# --- Optimizer and Loss ---\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "start_time = time.time()\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ------------------- TRAINING -------------------\n",
    "    model.train()\n",
    "    train_mse, train_cont = 0, 0\n",
    "    train_preds, train_true = [], []\n",
    "\n",
    "    print(f\"\\nüîπ Epoch {epoch + 1}/{num_epochs} - Training\")\n",
    "    for batch_images, batch_texts, batch_prices in tqdm(train_loader, desc=\"Training Progress\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        pred, text_proj, image_proj = model(batch_images, batch_texts)\n",
    "\n",
    "        mse = criterion(pred, batch_prices.to(device))\n",
    "        cont = contrastive_loss(text_proj, image_proj)\n",
    "\n",
    "        loss = mse + 0.5 * cont\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_mse += mse.item()\n",
    "        train_cont += cont.item()\n",
    "        train_preds.extend(pred.cpu().detach().numpy())\n",
    "        train_true.extend(batch_prices.cpu().numpy())\n",
    "\n",
    "    avg_train_mse = train_mse / len(train_loader)\n",
    "    avg_train_cont = train_cont / len(train_loader)\n",
    "    train_smape = smape(np.array(train_true), np.array(train_preds))\n",
    "    print(f\"‚úÖ Train MSE = {avg_train_mse:.4f}, Train Cont = {avg_train_cont:.4f}, Train SMAPE = {train_smape:.2f}%\")\n",
    "\n",
    "    # ------------------- TESTING -------------------\n",
    "    model.eval()\n",
    "    test_mse, test_cont = 0, 0\n",
    "    test_preds, test_true = [], []\n",
    "\n",
    "    print(f\"üß™ Epoch {epoch + 1}/{num_epochs} - Testing\")\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_texts, batch_prices in tqdm(test_loader, desc=\"Testing Progress\", leave=False):\n",
    "            pred, text_proj, image_proj = model(batch_images, batch_texts)\n",
    "            mse = criterion(pred, batch_prices.to(device))\n",
    "            cont = contrastive_loss(text_proj, image_proj)\n",
    "            test_mse += mse.item()\n",
    "            test_cont += cont.item()\n",
    "\n",
    "            if pred.dim() == 0:\n",
    "                test_preds.append(pred.cpu().numpy().item())\n",
    "            else:\n",
    "                test_preds.extend(pred.cpu().numpy())\n",
    "            test_true.extend(batch_prices.cpu().numpy())\n",
    "\n",
    "    avg_test_mse = test_mse / len(test_loader)\n",
    "    avg_test_cont = test_cont / len(test_loader)\n",
    "    test_smape = smape(np.array(test_true), np.array(test_preds))\n",
    "    print(f\"üìä Test MSE = {avg_test_mse:.4f}, Test Cont = {avg_test_cont:.4f}, Test SMAPE = {test_smape:.2f}%\")\n",
    "\n",
    "    # --- Save Model Per Epoch ---\n",
    "    model_save_path = os.path.join(DATASET_FOLDER, f'multimodal_model_epoch_{epoch + 1}.pth')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"üíæ Model saved for epoch {epoch + 1} at {model_save_path}\")\n",
    "\n",
    "# --- Final Message ---\n",
    "print(f\"\\nüèÅ Training completed in {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022c1a6c-10da-4ba7-a2f4-9fe9c03922ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders fine-tuning: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_19404\\1012793650.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"multimodal_model_epoch_6.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully for testing!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultimodalModel(fine_tune_encoders=False).to(device)\n",
    "model.load_state_dict(torch.load(\"multimodal_model_epoch_6.pth\", map_location=device))\n",
    "model.eval()\n",
    "print(\"‚úÖ Model loaded successfully for testing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7586af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74999 test samples\n"
     ]
    }
   ],
   "source": [
    "TEST_CSV = \"test/merged_test_with_image.csv\"\n",
    "IMAGE_FOLDER = \"test_images/test_images\"  # or wherever your images are\n",
    "\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "print(f\"Loaded {len(test_df)} test samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "889dbbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, df, image_folder):\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.text_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, row['image'])\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ö†Ô∏è Image not found: {image_path}, using blank placeholder\")\n",
    "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "\n",
    "        text = row['catalog_content_clean']\n",
    "        return img, text, row['sample_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc23170",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PredictionDataset(test_df, IMAGE_FOLDER)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebfc907a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/9375 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, texts, ids \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# Image preprocessing (inside forward call)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         pred, _, _ \u001b[38;5;241m=\u001b[39m model(images, texts)\n\u001b[0;32m      7\u001b[0m         preds \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:240\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    236\u001b[0m                 collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    238\u001b[0m             ]\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, texts, ids in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        # Image preprocessing (inside forward call)\n",
    "        pred, _, _ = model(images, texts)\n",
    "        preds = pred.cpu().numpy()\n",
    "        \n",
    "        for i, sid in enumerate(ids):\n",
    "            predictions.append({\"sample_id\": sid, \"predicted_price\": float(preds[i])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aead16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(predictions)\n",
    "output_path = \"/content/train_images/test_predictions.csv\"\n",
    "pred_df.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Predictions saved to {output_path}\")\n",
    "print(pred_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015438b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/vit-base-patch16-224/resolve/main/preprocessor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001CF7BB8B910>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 6e00979a-0b48-43b6-808d-df06768b9f88)')' thrown while requesting HEAD https://huggingface.co/google/vit-base-patch16-224/resolve/main/preprocessor_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/vit-base-patch16-224/resolve/main/preprocessor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001CF7BB8BD30>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 11d04167-fd6f-4d62-bcd2-e05ee3701d3a)')' thrown while requesting HEAD https://huggingface.co/google/vit-base-patch16-224/resolve/main/preprocessor_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/vit-base-patch16-224/resolve/main/preprocessor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001CF2A2D35B0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: b5ae37f3-3251-4275-ad08-793265311dcd)')' thrown while requesting HEAD https://huggingface.co/google/vit-base-patch16-224/resolve/main/preprocessor_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/vit-base-patch16-224/resolve/main/preprocessor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001CF2A2D3850>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: e51f4ba4-6f07-4b48-bb23-2d7b63cbc03a)')' thrown while requesting HEAD https://huggingface.co/google/vit-base-patch16-224/resolve/main/preprocessor_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders fine-tuning: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_12168\\374735848.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(MODEL_WEIGHTS_PATH, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from multimodal_model_epoch_6.pth (strict=False)\n",
      "Loaded 74999 rows\n",
      "Columns in CSV: ['sample_id', 'product_name', 'catalog_content_clean', 'value', 'unit', 'image']\n",
      "DataLoader ready with dataset structure.\n",
      "Error creating DataLoader: 'price'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting prices:   0%|          | 0/4688 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'price'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'price'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 155\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Predict and save prices\u001b[39;00m\n\u001b[0;32m    154\u001b[0m PREDICTIONS_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./predictions/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 155\u001b[0m \u001b[43mpredict_and_save_prices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPREDICTIONS_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 84\u001b[0m, in \u001b[0;36mpredict_and_save_prices\u001b[1;34m(model, data_loader, save_dir, csv_filename)\u001b[0m\n\u001b[0;32m     81\u001b[0m all_predicted_prices \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_images, batch_texts, _, batch_sample_ids, _ \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting prices\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;66;03m# Predict prices\u001b[39;00m\n\u001b[0;32m     87\u001b[0m             pred_price, _, _ \u001b[38;5;241m=\u001b[39m model(batch_images, batch_texts)\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 72\u001b[0m, in \u001b[0;36mProductDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     70\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     71\u001b[0m text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcatalog_content_clean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 72\u001b[0m price \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     73\u001b[0m sample_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, text, price, sample_id, image_path\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\harsh\\miniconda3\\envs\\battery\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'price'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTImageProcessor, ViTModel, BertTokenizer, BertModel, DistilBertModel\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# --- Multimodal Model ---\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, fine_tune_encoders=True):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.image_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.text_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.text_proj = nn.Linear(768, 512)\n",
    "        self.image_proj = nn.Linear(768, 512)\n",
    "        self.concat_proj = nn.Linear(1024, 768)\n",
    "        self.transformer = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if not fine_tune_encoders:\n",
    "            for param in self.image_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.text_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        print(\"Encoders fine-tuning:\", fine_tune_encoders)\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        inputs = self.image_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        image_outputs = self.image_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        image_proj = F.normalize(self.image_proj(image_outputs), dim=-1)\n",
    "        inputs = self.text_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "        text_outputs = self.text_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        text_proj = F.normalize(self.text_proj(text_outputs), dim=-1)\n",
    "        concat_emb = torch.cat([text_proj, image_proj], dim=-1)\n",
    "        concat_emb = self.concat_proj(concat_emb).unsqueeze(1)\n",
    "        transformer_out = self.transformer(inputs_embeds=concat_emb).last_hidden_state.mean(dim=1)\n",
    "        price = self.regression(transformer_out)\n",
    "        return price.squeeze(), text_proj, image_proj\n",
    "\n",
    "# --- Custom Dataset and Collate Function ---\n",
    "def custom_collate_fn(batch):\n",
    "    images, texts, prices, sample_ids, image_paths = zip(*batch)\n",
    "    return list(images), list(texts), torch.tensor(prices, dtype=torch.float32), list(sample_ids), list(image_paths)\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, df, image_folder):\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, row['image'])\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found at {image_path}, using placeholder.\")\n",
    "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        text = row['catalog_content_clean']\n",
    "        price = float(row['price'])\n",
    "        sample_id = row['sample_id']\n",
    "        return img, text, price, sample_id, image_path\n",
    "\n",
    "# --- Function to Predict and Save Prices ---\n",
    "def predict_and_save_prices(model, data_loader, save_dir, csv_filename='predictions.csv'):\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    all_sample_ids = []\n",
    "    all_predicted_prices = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_texts, _, batch_sample_ids, _ in tqdm(data_loader, desc=\"Predicting prices\"):\n",
    "            try:\n",
    "                # Predict prices\n",
    "                pred_price, _, _ = model(batch_images, batch_texts)\n",
    "                pred_price = pred_price.cpu().numpy()\n",
    "                \n",
    "                # Handle scalar or array outputs\n",
    "                if pred_price.ndim == 0:\n",
    "                    pred_price = [pred_price.item()]\n",
    "                else:\n",
    "                    pred_price = pred_price.tolist()\n",
    "                \n",
    "                # Store results\n",
    "                all_sample_ids.extend(batch_sample_ids)\n",
    "                all_predicted_prices.extend(pred_price)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    results = pd.DataFrame({\n",
    "        'sample_id': all_sample_ids,\n",
    "        'predicted_price': all_predicted_prices\n",
    "    })\n",
    "    output_path = os.path.join(save_dir, csv_filename)\n",
    "    results.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to {output_path}\")\n",
    "    print(f\"Total samples predicted: {len(results)}\")\n",
    "    return results\n",
    "\n",
    "# --- Main Script ---\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Initialize and load model\n",
    "MODEL_WEIGHTS_PATH = 'multimodal_model_epoch_6.pth'\n",
    "model = MultimodalModel(fine_tune_encoders=False).to(device)\n",
    "try:\n",
    "    # Load with strict=False to handle potential weight mismatches\n",
    "    state_dict = torch.load(MODEL_WEIGHTS_PATH, map_location=device)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Loaded weights from {MODEL_WEIGHTS_PATH} (strict=False)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading weights: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load data\n",
    "IMAGE_FOLDER = 'test_images/test_images'\n",
    "CSV_PATH = 'test/merged_test_with_image.csv'\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "    print(\"Columns in CSV:\", df.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Create DataLoader\n",
    "try:\n",
    "    dataset = ProductDataset(df, IMAGE_FOLDER)\n",
    "    data_loader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    print(\"DataLoader ready with dataset structure.\")\n",
    "    for batch_images, batch_texts, batch_prices, batch_sample_ids, batch_image_paths in data_loader:\n",
    "        print(f\"Batch loaded: {len(batch_images)} images, {len(batch_texts)} texts, {batch_prices.shape} prices\")\n",
    "        print(f\"Image type: {type(batch_images[0])}, Text type: {type(batch_texts[0])}, Price type: {type(batch_prices)}\")\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(f\"Error creating DataLoader: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Predict and save prices\n",
    "PREDICTIONS_DIR = './predictions/'\n",
    "predict_and_save_prices(model, data_loader, PREDICTIONS_DIR, csv_filename='predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d385b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders fine-tuning: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_6620\\1735015266.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(MODEL_WEIGHTS_PATH, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from multimodal_model_epoch_6.pth (strict=False)\n",
      "Loaded 74999 rows\n",
      "Columns in CSV: ['sample_id', 'product_name', 'catalog_content_clean', 'value', 'unit', 'image']\n",
      "DataLoader ready with dataset structure.\n",
      "Batch loaded: 16 images, 16 texts\n",
      "Image type: <class 'PIL.Image.Image'>, Text type: <class 'str'>, Sample ID type: <class 'numpy.int64'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting prices: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4688/4688 [1:34:01<00:00,  1.20s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictions saved to ./predictions/predictions.csv\n",
      "Total samples predicted: 74999\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sample_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "predicted_price",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a5a9cf69-9147-4139-af25-a2214a53df4b",
       "rows": [
        [
         "0",
         "100179",
         "23.52980613708496"
        ],
        [
         "1",
         "245611",
         "16.72756576538086"
        ],
        [
         "2",
         "146263",
         "37.904911041259766"
        ],
        [
         "3",
         "95658",
         "20.636444091796875"
        ],
        [
         "4",
         "36806",
         "28.833316802978516"
        ],
        [
         "5",
         "148239",
         "10.044278144836426"
        ],
        [
         "6",
         "92659",
         "14.24309253692627"
        ],
        [
         "7",
         "3780",
         "22.089618682861328"
        ],
        [
         "8",
         "196940",
         "37.12632369995117"
        ],
        [
         "9",
         "20472",
         "12.794772148132324"
        ],
        [
         "10",
         "121721",
         "30.348819732666016"
        ],
        [
         "11",
         "127336",
         "12.390467643737793"
        ],
        [
         "12",
         "20801",
         "68.79442596435547"
        ],
        [
         "13",
         "30103",
         "13.331536293029785"
        ],
        [
         "14",
         "68691",
         "28.151165008544922"
        ],
        [
         "15",
         "230703",
         "16.030006408691406"
        ],
        [
         "16",
         "3342",
         "12.08695125579834"
        ],
        [
         "17",
         "115635",
         "17.24964141845703"
        ],
        [
         "18",
         "221565",
         "9.324368476867676"
        ],
        [
         "19",
         "265927",
         "10.20030689239502"
        ],
        [
         "20",
         "290421",
         "13.557394981384277"
        ],
        [
         "21",
         "222203",
         "9.42963695526123"
        ],
        [
         "22",
         "40595",
         "10.190784454345703"
        ],
        [
         "23",
         "157931",
         "13.529007911682129"
        ],
        [
         "24",
         "41814",
         "17.624605178833008"
        ],
        [
         "25",
         "250478",
         "31.137981414794922"
        ],
        [
         "26",
         "130232",
         "22.213401794433594"
        ],
        [
         "27",
         "155825",
         "12.341368675231934"
        ],
        [
         "28",
         "42104",
         "11.502073287963867"
        ],
        [
         "29",
         "148748",
         "69.78485870361328"
        ],
        [
         "30",
         "247548",
         "28.943992614746094"
        ],
        [
         "31",
         "262167",
         "28.046802520751953"
        ],
        [
         "32",
         "100183",
         "17.221975326538086"
        ],
        [
         "33",
         "168985",
         "14.66811466217041"
        ],
        [
         "34",
         "18411",
         "69.3758316040039"
        ],
        [
         "35",
         "111287",
         "15.059165000915527"
        ],
        [
         "36",
         "156813",
         "9.347378730773926"
        ],
        [
         "37",
         "194861",
         "9.43717098236084"
        ],
        [
         "38",
         "104298",
         "23.04305648803711"
        ],
        [
         "39",
         "59033",
         "26.120807647705078"
        ],
        [
         "40",
         "31133",
         "21.169567108154297"
        ],
        [
         "41",
         "299318",
         "9.378135681152344"
        ],
        [
         "42",
         "255038",
         "9.447416305541992"
        ],
        [
         "43",
         "220578",
         "69.4665298461914"
        ],
        [
         "44",
         "38995",
         "25.42693328857422"
        ],
        [
         "45",
         "253068",
         "11.264801025390625"
        ],
        [
         "46",
         "282143",
         "18.330448150634766"
        ],
        [
         "47",
         "226531",
         "34.06094741821289"
        ],
        [
         "48",
         "233215",
         "41.031253814697266"
        ],
        [
         "49",
         "33295",
         "14.94885540008545"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 74999
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>predicted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100179</td>\n",
       "      <td>23.529806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245611</td>\n",
       "      <td>16.727566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146263</td>\n",
       "      <td>37.904911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95658</td>\n",
       "      <td>20.636444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36806</td>\n",
       "      <td>28.833317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74994</th>\n",
       "      <td>93616</td>\n",
       "      <td>12.656566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74995</th>\n",
       "      <td>249434</td>\n",
       "      <td>14.198102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74996</th>\n",
       "      <td>162217</td>\n",
       "      <td>9.435763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74997</th>\n",
       "      <td>230487</td>\n",
       "      <td>27.267073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74998</th>\n",
       "      <td>279477</td>\n",
       "      <td>11.807330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74999 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id  predicted_price\n",
       "0         100179        23.529806\n",
       "1         245611        16.727566\n",
       "2         146263        37.904911\n",
       "3          95658        20.636444\n",
       "4          36806        28.833317\n",
       "...          ...              ...\n",
       "74994      93616        12.656566\n",
       "74995     249434        14.198102\n",
       "74996     162217         9.435763\n",
       "74997     230487        27.267073\n",
       "74998     279477        11.807330\n",
       "\n",
       "[74999 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTImageProcessor, ViTModel, BertTokenizer, BertModel, DistilBertModel\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# --- Multimodal Model ---\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, fine_tune_encoders=True):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.image_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.text_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.text_proj = nn.Linear(768, 512)\n",
    "        self.image_proj = nn.Linear(768, 512)\n",
    "        self.concat_proj = nn.Linear(1024, 768)\n",
    "        self.transformer = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if not fine_tune_encoders:\n",
    "            for param in self.image_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.text_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        print(\"Encoders fine-tuning:\", fine_tune_encoders)\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        inputs = self.image_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        image_outputs = self.image_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        image_proj = F.normalize(self.image_proj(image_outputs), dim=-1)\n",
    "        inputs = self.text_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "        text_outputs = self.text_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        text_proj = F.normalize(self.text_proj(text_outputs), dim=-1)\n",
    "        concat_emb = torch.cat([text_proj, image_proj], dim=-1)\n",
    "        concat_emb = self.concat_proj(concat_emb).unsqueeze(1)\n",
    "        transformer_out = self.transformer(inputs_embeds=concat_emb).last_hidden_state.mean(dim=1)\n",
    "        price = self.regression(transformer_out)\n",
    "        return price.squeeze(), text_proj, image_proj\n",
    "\n",
    "# --- Custom Dataset and Collate Function ---\n",
    "def custom_collate_fn(batch):\n",
    "    images, texts, sample_ids, image_paths = zip(*batch)\n",
    "    return list(images), list(texts), list(sample_ids), list(image_paths)\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, df, image_folder):\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, row['image'])\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found at {image_path}, using placeholder.\")\n",
    "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        text = row['catalog_content_clean']\n",
    "        sample_id = row['sample_id']\n",
    "        return img, text, sample_id, image_path\n",
    "\n",
    "# --- Function to Predict and Save Prices ---\n",
    "def predict_and_save_prices(model, data_loader, save_dir, csv_filename='predictions.csv'):\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    all_sample_ids = []\n",
    "    all_predicted_prices = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_texts, batch_sample_ids, _ in tqdm(data_loader, desc=\"Predicting prices\"):\n",
    "            try:\n",
    "                pred_price, _, _ = model(batch_images, batch_texts)\n",
    "                pred_price = pred_price.cpu().numpy()\n",
    "                \n",
    "                if pred_price.ndim == 0:\n",
    "                    pred_price = [pred_price.item()]\n",
    "                else:\n",
    "                    pred_price = pred_price.tolist()\n",
    "                \n",
    "                all_sample_ids.extend(batch_sample_ids)\n",
    "                all_predicted_prices.extend(pred_price)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'sample_id': all_sample_ids,\n",
    "        'predicted_price': all_predicted_prices\n",
    "    })\n",
    "    output_path = os.path.join(save_dir, csv_filename)\n",
    "    results.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to {output_path}\")\n",
    "    print(f\"Total samples predicted: {len(results)}\")\n",
    "    return results\n",
    "\n",
    "# --- Main Script ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize and load model\n",
    "MODEL_WEIGHTS_PATH = 'multimodal_model_epoch_6.pth'\n",
    "model = MultimodalModel(fine_tune_encoders=False).to(device)\n",
    "try:\n",
    "    state_dict = torch.load(MODEL_WEIGHTS_PATH, map_location=device)\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Loaded weights from {MODEL_WEIGHTS_PATH} (strict=False)\")\n",
    "    if missing_keys:\n",
    "        print(f\"Missing keys: {missing_keys}\")\n",
    "    if unexpected_keys:\n",
    "        print(f\"Unexpected keys: {unexpected_keys}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading weights: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load data\n",
    "IMAGE_FOLDER = 'test_images/test_images'\n",
    "CSV_PATH = 'test/merged_test_with_image.csv'  # Update to your test CSV path\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "    print(\"Columns in CSV:\", df.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Create DataLoader\n",
    "try:\n",
    "    dataset = ProductDataset(df, IMAGE_FOLDER)\n",
    "    data_loader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    print(\"DataLoader ready with dataset structure.\")\n",
    "    for batch_images, batch_texts, batch_sample_ids, batch_image_paths in data_loader:\n",
    "        print(f\"Batch loaded: {len(batch_images)} images, {len(batch_texts)} texts\")\n",
    "        print(f\"Image type: {type(batch_images[0])}, Text type: {type(batch_texts[0])}, Sample ID type: {type(batch_sample_ids[0])}\")\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(f\"Error creating DataLoader: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Predict and save prices\n",
    "PREDICTIONS_DIR = './predictions/'\n",
    "predict_and_save_prices(model, data_loader, PREDICTIONS_DIR, csv_filename='predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5ee8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "battery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
